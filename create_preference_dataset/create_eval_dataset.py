import os
import pickle
import random
import json
import argparse

def delete_comments(code: str):
    without_comms = ""
    for line in code.split("\n"):
        if line.strip().startswith("#"):
            continue
        without_comms += line + "\n"
    return without_comms
    

def sample_chosen_rejected(path_to_test_results:str, path_to_codes:str, problem):
    """
    Returns a dictionary of the following form:
    {
     "chosen": "*code with the highest score*",
     "rejected": "*a random code that does not have the highest score*"
    }
    """
    results_and_sols = []
    with open(os.path.join(path_to_codes, f"{problem}.json"), "r") as f:
        codes = json.load(f)
    with open(os.path.join(path_to_test_results, f"{problem}.pkl"), "rb") as f:
        test_results = pickle.load(f)

    for (result, sol) in zip(test_results[int(problem)]["results"], codes[problem]["code"]):
        results_and_sols.append({"result": result, "code": sol})

    chosen_rejected = []
    for i in range(args.samples_per_task):
        results_and_sols.sort(key=lambda x: x["result"], reverse=True)
        if results_and_sols[0]["result"] == results_and_sols[-1]["result"]:
            return None
        if results_and_sols[0]["result"] < args.best_threshold and i == 0:
            return None
        chosen = results_and_sols[0]["code"]

        rejected_candidates = [result for result in results_and_sols[1:] if result["result"] != args.best_threshold]

        rejected = random.choice(rejected_candidates)["code"]
        results_and_sols = [res for res in results_and_sols if res["code"] != chosen and res["code"] != rejected]
        chosen = delete_comments(chosen)
        rejected = delete_comments(rejected)
        chosen_rejected.append({"chosen": chosen, "rejected": rejected})
    return chosen_rejected


def create_dpo_dataset_dict():
    """
    Creates a preference dataset in the DPO-conform format.
    """
    random.seed(9054358)
    prompts = []
    chosen_output = []
    rejected_output = []
    max_len = args.max_len
    problems = os.listdir(args.path_to_test_results)
    print(len(problems))
    random.shuffle(problems)
    for problem in problems:
        problem = problem[:-4]
        problem = "0"*(4-len(problem)) + problem
        print("before")
        with open(
            os.path.join(args.path_to_data, problem, "question.txt"),
            "r",
            encoding="utf-8",
        ) as f:
            prompt = f.read()
            print("in")
        print("after")
        problem = str.lstrip(problem, "0")
        try:
            sample = sample_chosen_rejected(args.path_to_test_results, args.path_to_codes, problem)
            if sample is None:
                continue
            chosen_output.extend([s["chosen"] for s in sample])
            rejected_output.extend([s["rejected"] for s in sample])
            print(len(sample))
            for _ in range(len(sample)):
                prompts.append(prompt)
            if len(prompts) == max_len * args.samples_per_task:
                print("Here?")
                break
        except Exception as e:
            print(e)

    eval_prompts = []
    eval_chosen_output = []
    eval_rejected_output = []
    print(len(prompts))
    indexes = random.sample(range(len(prompts)), k=100)
    print(len(indexes))
    print(sorted(indexes))

    for index, (pr, ch, rej) in enumerate(zip(prompts, chosen_output, rejected_output)):
        if index in indexes:
            eval_prompts.append(pr)
            eval_chosen_output.append(ch)
            eval_rejected_output.append(rej)

    for (pr, ch, rej) in zip(eval_prompts, eval_chosen_output, eval_rejected_output):
            prompts.remove(pr)
            chosen_output.remove(ch)
            rejected_output.remove(rej)


    dpo_dataset_dict = {
        "prompt": prompts,
        "chosen": chosen_output,
        "rejected": rejected_output,
    }
    eval_dataset_dict = {
        "prompt": eval_prompts,
        "chosen": eval_chosen_output,
        "rejected": eval_rejected_output,
    }
    print(len(dpo_dataset_dict["prompt"]), len(eval_dataset_dict["prompt"]))
    return dpo_dataset_dict, eval_dataset_dict


def main(args):
    """
    Creates a DPO dataset and saves it into a file.
    """
    dpo_dataset_dict, eval_dataset_dict = create_dpo_dataset_dict()
    with open(args.path_to_dpo, "w", encoding="utf-8") as f:
        json.dump(dpo_dataset_dict, f)
    with open(args.path_to_eval, "w", encoding="utf-8") as f:
        json.dump(eval_dataset_dict, f)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-pd", "--path_to_data", help="Path to APPS dataset")
    parser.add_argument("-pt", "--path_to_test_results", 
                        help="Path to test results of codes generated by the model")
    parser.add_argument("--path_to_eval", help="path to json with eval dataset")
    parser.add_argument("-pdpo", "--path_to_dpo", help="Path to future DPO dataset")
    parser.add_argument("-bt", "--best_threshold", type=float, help="Threshold for the best code. Value from 0 to 1")
    parser.add_argument("-pc", "--path_to_codes", help="path to generated codes")
    parser.add_argument("--max_len", type=int, help="Length of the DPO dataset")
    parser.add_argument("--samples_per_task", type=int, help="How many examples to take per one task")
    args = parser.parse_args()
    main(args)
